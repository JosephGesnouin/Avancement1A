\clearpage
\section{Experimentations, Réalisations}
\label{sec:SOTA}

\subsection{Squeletisation}
Afin de pouvoir réaliser des premiers tests  et pour avoir une première expérience des problèmes rencontrés, j'ai utilisé des versions de l'état de l'art des approches de classifications 

\subsection{Représentation Graphe/Squelette porteuse de sens}
Le format de structure de données de la majorité des articulations disponibles via les approches de squelettisation en un tenseur ignore les relations de dépendance physique entre les articulations et ajoute de fausses connexions entre les articulations du corps qui ne sont pas liées physiquement.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.3\linewidth]{Images/openpose.png}
    \caption{Un exemple de la représentation squelette obtenue à l'aide de la bibliothèque OpenPose \cite{cao2017realtime}}
    \label{fig:openPoseSkel}
\end{figure}

La figure \ref{fig:openPoseSkel}, nous permet de réaliser que conserver le squelette obtenu par les algorithmes de detection de pose classiques sans avoir recourt à une transformation classique pourrait biaiser nos résultats: certains couples d'articulations, bien que se suivant de manière incrémentale dans la structure de données utilisée, n'ont en réalité aucune raison valable de l'être:



\subsection{Autoencodeur semi-supervisé pour l'action recognition}

\subsubsection{Introduction}
Le deep learning étant une composition de transformation non linéaires en esperant trouver une représentation adaptée au format des données et conservant un maximum de sémantique de celui-ci. (représentation dans l'espace où \textbf{les instances peuvent être séparées linéairement}.)

Je me suis intéréssé à cette question de représentation pour plusieurs raisons:
\begin{itemize}
    \item Tout est une question d'embedding et de normalisation en machine learning. Au final, jouer avec des layers permet seulement de transformer cet embedding en espérant qu'il soit porteur d'information. Le jour où l'on trouve un moyen de normaliser et de représenter les données de manière plus adequate, n'importe quel classifieur pourra obtenir de bons résultats pour peu que les données d'entrée soient porteuses d'information. Le jour ou l'on arrivera à obtenir une représentation utilisable plus rapidement au lieu de stacker des layers au sein d'un réseau de neurones, d'autres problématiques d'apprentissage apparaitront mais le gain serait intéréssant: (Théoriquement n'importe quel Shallow Network peut supposément approximer une fonction donnée \cite{2016arXiv160803287M}).
    
    \item En s'intéréssant à la question de la représentation des données, on évite un apprentissage de cette représentation à "l'aveugle" (en optimisant un réseau précis sur un jeu de données précis et en testant un maximum d'hyperparamètres on est assurés de faire un bon résultat.  On peut \textbf{potentiellement} réduire la taille de notre réseau et donc par définition réduire le temps d'inférence de celui-ci, ce qui peut être intéréssant dans le cadre d'un sujet en temps réel.
\end{itemize}

L'idée étant d'entrainer un autoencodeur avec une fonction de coût modifiée en rajoutant une contrainte sur la séparation linéaire des classes dans l'espace latent (LDA / QDA / FDA). Il a été montré que la représentation optimale interne d'un MLP s'obtient en faisant une analogie avec une analyse discriminante non linéaire \cite{webb1990optimised}.

Les approches factorielles citées plus haut s'apparentent à la projection de données dans l'espace comme celle d'une ACP mais tandis que l'ACP maximise la variance du jeu de données, celles-ci se focalisent sur la maximisation de la séparabilité des classes dans l'espace (supervisé). On obtiendrait donc un AutoEncodeur "semi-supervisé" dans le sens où celui-ci se focalise sur deux informations complètement différentes dans les données, l'une de manière non-supervisée, l'autre de manière supervisée:
\begin{itemize}
    \item La structure inhérente des données capturée de manière non supervisée grâce à l'Auto-encodeur, on conserverait une partie de l'information importante et discriminante du jeu de données. (feature extraction)
    \item La séparabilité des classes dans l'espace grâce à l'analyse discriminante (non) linéaire. Ce qui permettrait de réduire le nombre de layer au final.
\end{itemize}{}


Une fois la convergence obtenue, on peut imaginer deux utilisations:
\begin{itemize}
    \item Récupérer cet espace latent combinant les informations "data driven" grâce à l'apprentissage non-supervisé de l'auto encodeur classique et une première ébauche de séparabilité linéaire grâce à la seconde partie de la fonction de coût. Envoyer l'espace latent à un input de CNN classique très peu profond, GAP, Softmax ou un MLP, Softmax. Finetuner le réseau de bout en bout et voir ce que cela donne quand à la capacité de discrimination de l'approche. 
    \item Générer des données en réalisant une génération d'instance au niveau de l'espace latent et utiliser la partie décodeur pour voir si cela donne des instances "utilisables" pour faire de la data génération.
\end{itemize}

\subsubsection{Formalisation}
On pose le problème comme ceci: \newline
$$\min _{\theta_{1}, \theta_{2}},\left\|\mathbf{X}-g_{\theta_{2}}\left(f_{\theta 1}(\mathbf{X})\right)\right\|^{2}$$

Fonction de coût classique d'un autoencodeur avec $\theta_{1}, \theta_{2}$ les parametres des blocs encodeur et decodeur de l’AE. 

$$\min _{\theta_{1}, \theta_{2}, \mathbf{S}}\left\|\mathbf{X}-g_{\theta_{2}}\left(f_{\theta_{1}}(\mathbf{X})\right)\right\|^{2}+\lambda\left\|f_{\theta_{1}}(\mathbf{X})-\mathbf{S}_{f_{\theta 1}}(\mathbf{X})\right\|^{2}$$

Ajout d'une variation dans la fonction de coût: avec S la matrice de projection des individus dans l'espace latent obtenue avec une analyse discriminante (LDA / QDA / FDA).

\subsubsection{Tests et évaluation}
J'ai réalisé l'implémentation de cette approche sur deux jeux de données: SHREC

Le modèle final finetuné est assez trivial: un MLP à 0.13M de paramètres  qui comporte la partie encodeur et une softmax 


Au final, on obtient un modèle à 88.6\% d'accuracy (23k classification par seconde) donc très rapide comparé aux autres approches, même si l'accuracy laisse à désirer. A titre de comparaison, j'ai réalisé la même pipeline avec un AE MLP et une fonction de coût habituelle, je tombe sur 86.2\% donc j'imagine qu'il y'a un petit truc à creuser, pas nécéssairement sur la course à l'accuracy mais sur la vitesse de l'approche.

\subsubsection{Remarques}
\begin{itemize}
    \item le nombre maximum d'axes obtenus par la LDA correspond à (\#classes-1): sur SHREC 14 l'espace latent est donc de dimension 13 toute représentation avec un espace latent superieur est impossible (on ne peut donc difficilement imposer des contraintes de sparsité au modèle)
    \item Problème de convergence au niveau de l'apprentissage de l'AE avec fonction de cout modifiée (optimiser ADAM), j'imagine que mon learning rate est trop élevé mais il faudrait pousser plus profondément afin de voir pourquoi l'optima de la solution du problème est aussi difficile à obtenir.
    \item optimisation de $\lambda$ qui devient un hyperparamètre
\end{itemize}

A creuser:\\
Essayer plusieurs structures plus travaillées qu'un simple MLP:
\begin{itemize}
    \item AE: Convolutions / Attention / Séquentiel / VAE.
    \item analyse discriminante: QDA  pour ne pas émettre de suppositions vis-à-vis des matrices de covariance de chaque classes
\end{itemize}

Rien n'est fixé dans la fonction de coût, j'imagine qu'ajouter un autre critère de régularization ne peut être que bénéfique, mais vu les prolèmes de convergence actuels, je préfère ne pas le tenter pour l'instant.

