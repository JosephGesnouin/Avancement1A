\clearpage
\chapter{Experimentations, Réalisations}
\label{sec:SOTA}

\section{Squeletisation}
Afin de pouvoir réaliser des premiers tests  et pour avoir une première expérience des problèmes rencontrés, j'ai utilisé des versions de l'état de l'art des approches de classifications 

\section{Représentation Graphe/Squelette porteuse de sens}
Le format de structure de données de la majorité des articulations disponibles via les approches de squelettisation en un tenseur ignore les relations de dépendance physique entre les articulations et ajoute de fausses connexions entre les articulations du corps qui ne sont pas liées physiquement.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.3\linewidth]{Images/openpose.png}
    \caption{Structure de données de la représentation squelette obtenue à l'aide de la bibliothèque OpenPose \cite{cao2017realtime}}
    \label{fig:openPoseSkel}
\end{figure}

La figure \ref{fig:openPoseSkel}, nous permet de constater que conserver le squelette obtenu par les algorithmes de detection de pose classiques, sans avoir recours à une transformation pourrait réduire la qualité de nos résultats: certains couples d'articulations, bien que se suivant de manière incrémentale dans la structure de données utilisée, n'ont en réalité aucune raison valable de l'être: par exemple, l'extrémité gauche du bras et l'épaule droite (\textit{nodes 4 et 5}) ou encore l'extrémité droite du bras et la hanche gauche (\textit{nodes 7 et 8}).

Une grande majorité des travaux actuels semblent négliger l'importance de cette représentation spatiale et ne se focalisent que sur le coté temporel: la dynamique des articulations, sans remettre en question la structure de données spatio-temporelle en entrée.

En m'inspirant des travaux de \cite{liu2016spatio} et \cite{2018arXiv180110304Y}, j'ai souhaité m'intérésser à cette question de représentation: en réalisant un Depth-First Search (DFS) sur un hub du graphe, il est possible d'obtenir une structure de type arbre/graphe\textbf{ ..... QUALIFICATIF.}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{Images/DFS.png}
    \caption{(a) Articulations du squelette d'un corps humain avec la structure de données initiale. L'ordre de visite des noeuds est incrémentale:1-2-3-...-16. (b) Le squelette est transformé en une structure arborescente. (c)  L'arbre peut être dépilé en une chaîne dont l'ordre de visite des noeuds conserve la relation physique des articulations: 1-2-3-2-4-5-6-5-4-2-7-8-9-8-7-2-1-10-11-12-13-12-11-10-14-15-16-15-14-10-1, Figure équivalente à la figure 2 dans  \cite{liu2016spatio}.}
    \label{fig:DFS}
\end{figure}

 Double-feature Double-motion Network (DD-Net) \cite{2019arXiv190709658Y}

\begin{table}[H]
\centering
\scalebox{0.75}{
\begin{tabular}{l|l|l|l} 
\hline
\textbf{Méthode }                                                       & \textbf{Paramètres}  & \textbf{14 classes } & \textbf{28 classes }  \\ 
\hline
DD-Net (filtres \= 64, w/o global fast \& slow motion)     & 1.7M                 & 55.2\%               & 41.6\%                \\
DD-NET (filtres \= 64)                                                   & 1.82M                & 94.6\%               & 91.9\%                \\
DD-NET (filtres \= 32)                                                   & 0.50M                & 93.5\%               & 90.4\%                \\
DD-NET (filtres \= 16)                                                   & 0.15M                & 91.8\%               & 90.0\%                \\
                                                                        &                      &                      &                       \\ 
\hline
DFS-DD-Net (filtres \= 64, w/o global fast \& slow motion) & M                    & \%                   & \%                    \\
DFS-DD-NET (filtres \= 64)                                               & 1.84M                    & 95.7\%                   & \%                    \\
DFS-DD-NET (filtres \= 32)                                               & 0.51M                    & 95.5\%                   & 93.9\%                    \\
DFS-DD-NET (filtres \= 16)                                               & 0.16M                & 93.1\%                   & 92.5\%                    \\
\hline
\end{tabular}
}
\end{table}

\textbf{RESULTATS SUR JHMDB}


\section{Autoencodeur semi-supervisé pour l'action recognition}

\subsection{Introduction}
Les auto encodeurs étant une composition de transformation non linéaires en esperant trouver une représentation dans l'espace adaptée au format des données et conservant un maximum de sémantique de celui-ci.

Je me suis intéréssé à cette question de représentation pour plusieurs raisons:
\begin{itemize}
    \item Tout est une question d'embedding et de normalisation en machine learning. Au final, le role premier des couches cachées est de transformer cet embedding en espérant qu'il soit porteur d'information. Le jour où l'on trouve un moyen de normaliser et de représenter les données de manière plus adequate, n'importe quel classifieur pourra obtenir de bons résultats pour peu que les données d'entrée soient porteuses d'information. Le jour ou l'on arrivera à obtenir une représentation utilisable plus rapidement au lieu de stacker des layers au sein d'un réseau de neurones, d'autres problématiques d'apprentissage apparaitront mais le gain serait intéréssant: (Selon le théorème d'approximation universelle, n'importe quel Shallow Network peut supposément approximer une fonction donnée \cite{universalapproxtheorm,scarselli1998universal}).
    
    \item En s'intéréssant à la question de la représentation des données, on évite un apprentissage de cette représentation à "l'aveugle" (en optimisant un réseau précis sur un jeu de données précis et en testant un maximum d'hyperparamètres on est assurés de faire un bon résultat.  On peut potentiellement réduire la taille de notre réseau et donc par définition réduire le temps d'inférence de celui-ci, ce qui peut être intéréssant dans le cadre d'un sujet en temps réel.
\end{itemize}

L'idée étant d'entrainer un autoencodeur avec une fonction de coût modifiée en rajoutant une contrainte sur la séparation linéaire des classes dans l'espace latent (LDA / QDA / FDA). Il a été montré que la représentation optimale interne d'un MLP s'obtient en faisant une analogie avec une analyse discriminante non linéaire \cite{webb1990optimised}.

Les approches factorielles citées plus haut s'apparentent à la projection de données dans l'espace comme celle d'une ACP mais tandis que l'ACP maximise la variance du jeu de données, celles-ci se focalisent sur la maximisation de la séparabilité des classes dans l'espace (supervisé). On obtiendrait donc un AutoEncodeur "semi-supervisé" dans le sens où celui-ci se focalise sur deux informations complètement différentes dans les données, l'une de manière non-supervisée, l'autre de manière supervisée:
\begin{itemize}
    \item La structure inhérente des données capturée de manière non supervisée grâce à l'Auto-encodeur, on conserverait une partie de l'information importante et discriminante du jeu de données. (feature extraction)
    \item La séparabilité des classes dans l'espace grâce à l'analyse discriminante (non) linéaire. Ce qui permettrait de réduire le nombre de layer au final.
\end{itemize}{}


Une fois la convergence obtenue, on peut imaginer deux utilisations:
\begin{itemize}
    \item Récupérer cet espace latent combinant les informations "data driven" grâce à l'apprentissage non-supervisé de l'auto encodeur classique et une première ébauche de séparabilité linéaire grâce à la seconde partie de la fonction de coût. Envoyer l'espace latent à un input de CNN classique très peu profond, GAP, Softmax ou un MLP, Softmax. Finetuner le réseau de bout en bout et voir ce que cela donne quand à la capacité de discrimination de l'approche. 
    \item Générer des données en réalisant une génération d'instance au niveau de l'espace latent grâce à des approches type SMOTE \cite{chawla2002smote} ou ADASYN \cite{he2008adasyn}  et utiliser la partie décodeur pour  faire de la data génération.
\end{itemize}

\subsection{Formalisation}
On pose le problème comme ceci: \newline
$$\min _{\theta_{1}, \theta_{2}},\left\|\mathbf{X}-g_{\theta_{2}}\left(f_{\theta 1}(\mathbf{X})\right)\right\|^{2}$$

Fonction de coût habituelle d'un autoencodeur avec $\theta_{1}, \theta_{2}$ les parametres des blocs encodeur et decodeur de l’AE. 

$$\min _{\theta_{1}, \theta_{2}, \mathbf{S}}\left\|\mathbf{X}-g_{\theta_{2}}\left(f_{\theta_{1}}(\mathbf{X})\right)\right\|^{2}+\lambda\left\|f_{\theta_{1}}(\mathbf{X})-\mathbf{S}_{f_{\theta 1}}(\mathbf{X})\right\|^{2}$$

Ajout d'une variation dans la fonction de coût: avec S la matrice de projection des individus dans l'espace latent obtenue avec une analyse discriminante (LDA / QDA / FDA).

\subsection{Tests et évaluation}
J'ai réalisé l'implémentation de cette approche sur deux jeux de données: SHREC \cite{de2017shrec}

Le modèle final finetuné est assez trivial: un MLP à 0.13M de paramètres comportant la partie encodeur de l'autoencodeur et une softmax

Au final, on obtient un modèle à 88.6\% d'accuracy (23k classification par seconde) donc très rapide comparé aux autres approches, même si l'accuracy laisse à désirer. A titre de comparaison, j'ai réalisé la même pipeline avec un AE MLP et une fonction de coût habituelle, je tombe sur 86.2\% donc j'imagine qu'il y'a un petit truc à creuser, pas nécéssairement sur la course à l'accuracy mais sur la vitesse de l'approche.

\subsection{Remarques}
\begin{itemize}
    \item Le nombre maximum d'axes obtenus par la LDA correspond au nombre de classe moins un: sur SHREC 14 l'espace latent est donc de dimension 13 toute représentation avec un espace latent superieur est impossible (on ne peut donc difficilement imposer des contraintes de sparsité au modèle). Cela implique également une perte très lourde de l'information vis-à-vis de l'information dans l'espace latent. (SHREC: de 2122 neurones d'entrée à un espace latent avec 13 neurones)
    \item Quelques problèmes de convergence dûs à l'optimisation de $\lambda$ durant l'apprentissage de l'autoencodeur avec la fonction de coût modifiée(optimiser ADAM \cite{kingma2014adam}).
\end{itemize}

A creuser:\\
Essayer plusieurs structures plus travaillées qu'un simple MLP:
\begin{itemize}
    \item AE: Convolutions / Attention / Séquentiel / VAE.
    \item analyse discriminante: QDA  pour ne pas émettre de suppositions vis-à-vis des matrices de covariance de chaque classes
\end{itemize}

Rien n'est fixé dans la fonction de coût, j'imagine qu'ajouter un autre critère de régularization ne peut être que bénéfique, mais vu les prolèmes de convergence actuels, je préfère ne pas le tenter pour l'instant.

