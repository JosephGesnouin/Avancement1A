\clearpage
\chapter{Conclusion}
\label{sec:SOTA}

Au cours de ces huit mois de thèse, un état de l'art a été effectué pour plusieurs domaines de recherche dans l'optique de les combiner sous la forme de briques indépendantes afin de répondre à la problématique actuelle du sujet de thèse : «Peut-on lier la dynamique des articulations d'un piéton à une intention ? »\\


\textbf{L'estimation de pose}, nécessaire étape d'une prédiction dont l'analyse de la posture est un composant essentiel. Nous avons présenté les deux types d'approches pour la squelettisation: Bottom up et Top Down. La robustesse des approches Top Down comparé aux approches Bottom up et la capacité à pouvoir minimiser leur temps d'inférence dans le cadre spécifique de nos travaux nous pousse à sélectionner une méthode de l'état de l'art de ce type afin de squelettiser les piétons. Nous avons ensuite étudié les questions relatives à la dimension du squelette obtenu et l'utilisation des informations séquentielles pour obtenir une meilleure inférence. Tandis que les approches de squelettisation 3D sont moins abouties de par la faible quantité de jeux de données annotés, la recherche sur la question de la séquentialité pour l'estimation de pose semble plus aboutie. Nous avons donc pour l'instant décidé de nous focaliser sur une approche Top Down prenant en compte la séquentialité à base d'appariement de pose. Cela afin de pouvoir réaliser un meilleur suivi des protagonistes de la scène et de ne pas mélanger les dynamiques de deux protagonistes probablement du à un changement d'angle de caméra. A plus long terme, il est néanmoins envisagé d'adapter une approche de l'état de l'art de ce type afin de l'associer à un modèle d'inférence 3D pour régler des problèmes que nous pourrions rencontrer tels que les problèmes d'occlusion dans une scène bondée.   \\


\textbf{La reconnaissance d'actions squelettiques}, afin d'obtenir une idée générale des types d'approches utilisés dans l'état de l'art pour ce format de donnée. Comparé aux approches dites classiques d'apprentissage profond pour le traitement de données séquentielles, les convolutions semblent toutes aussi légitimes quand à leur utilisation pour cette tache. De plus, leur capacité à converger plus vite et à pouvoir traiter des entrées de longueur variable en fait une approche de choix dans le cadre de nos travaux où la vitesse d'inférence est importante. Il est intéressant de constater que la majorité des approches semblent minimiser l'importance de la cohérence d'une structure de données spatio-temporelle respectant les relations de dépendance physique. Par conséquent, nous souhaitons actuellement axer notre recherche dans ce domaine et construire une représentation de l'information permettant une justification du conditionnement des réseaux et une explicabilité des résultats conservant ces relations de dépendance physique. Que cela soit en utilisant une normalisation en entrée sur les données ou en utilisant de l'attention dite spatiale, afin d'aider le réseau de neurones à sélectionner, ou à pondérer différemment, les parties du corps avant d'analyser les évolutions temporelles des articulations. \\


\textbf{La prédiction d'intention}, laissant apparaître d'autres problématiques comparé à la reconnaissance d'actions seule : la nécessité de réaliser une inférence sur une séquence incomplète. Nous distinguons deux types de prédiction: continue et discrète (à court et long terme). Tandis que les inférences à long terme sont un aspect qui ne sera pas approfondi d'avantage de par la nature de nos travaux sur des séquences assez courtes, le conditionnement des actions en fonction d'un objet présent dans la scène peut-être une piste à approfondir pour améliorer la qualité du modèle.
L'inférence à court terme d'une action peut être traitée de plusieurs manière: ajouter une contrainte à la fonction de coût du réseau pour prioriser les premiers pas de temps ou utiliser sur des méthodes génératives de l'état de l'art. Nous allons pour l'instant prioriser la première approche afin de ne pas alourdir le temps d'inférence du modèle car l'adaptation d'une fonction de coût n'influencera en rien le temps d'inférence d'un modèle comparé aux méthodes génératives qui peuvent être gourmandes en temps de calcul. Cela afin de conserver à l'esprit la contrainte majeur de notre travail: la contrainte temps réel.
Nous nous orientons donc notre recherche vers la création d'une architecture de traitement via réseaux de neurones multimodales prenant en charge des dynamiques à périodes courtes ainsi que des méta-informations supplémentaires en entrée. L'approche pour l'inférence continue sera par exemple conditionnée en ajoutant la classe discrète d'action du piéton sous la forme d'une méta information au même titre que d'autres données mixtes (squelette, image, informations qualitatives: contexte) avec une fonction de coût modifiée pour prioriser les premiers pas de temps et classifier correctement l'action le plus rapidement possible. \\